{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNTAuMZ6ZJhIhIFS1gpu5Ok",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/k-dinakaran/Intelligent-Hiring-Assistant-Chatbot-for-Recruitment-Agency/blob/main/hiring_chat_bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljPAVGdSb8TM",
        "outputId": "c78c3737-09bc-44de-a84c-dc7ddbdcf756"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To log in, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: fineGrained).\n",
            "The token `emotional sentiment analysis` has been saved to /root/.cache/huggingface/stored_tokens\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful.\n",
            "The current active token is: `emotional sentiment analysis`\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio transformers torch accelerate bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3ZCuuKSb-kC",
        "outputId": "72251d9c-042d-4cbf-a341-1c5c65106a64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.23.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.5.2)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.45.4)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.12)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.5.0)\n",
            "Requirement already satisfied: gradio-client==1.8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.8.0)\n",
            "Requirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.29.3)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\n",
            "Requirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.0.2)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.16)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.0)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.11.3)\n",
            "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.1)\n",
            "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.2)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.13.0)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (2025.3.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.8.0->gradio) (15.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.0)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "import json\n",
        "from datetime import datetime\n",
        "\n",
        "# Initialize model and conversation state\n",
        "model, tokenizer = None, None\n",
        "conversation_state = {\n",
        "    'user_data': {},\n",
        "    'current_step': 0,\n",
        "    'generated_questions': [],\n",
        "    'question_answers': [],\n",
        "    'awaiting_answers': False\n",
        "}\n",
        "\n",
        "information_steps = [\n",
        "    (\"What is your full name?\", \"name\"),\n",
        "    (\"Please provide your email address:\", \"email\"),\n",
        "    (\"What is your phone number?\", \"phone\"),\n",
        "    (\"How many years of experience do you have? (0-1, 1-3, 3-5, 5+)\", \"experience\"),\n",
        "    (\"What position are you applying for?\", \"position\"),\n",
        "    (\"Where are you currently located?\", \"location\"),\n",
        "    (\"Please list your technical skills (comma separated, e.g., Python, SQL):\", \"tech_stack\")\n",
        "]\n",
        "\n",
        "def load_model():\n",
        "    global model, tokenizer\n",
        "    if model is None:\n",
        "        model_name = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            device_map=\"auto\",\n",
        "            load_in_4bit=True\n",
        "        )\n",
        "    return model, tokenizer\n",
        "\n",
        "def generate_questions(tech_stack, experience):\n",
        "    model, tokenizer = load_model()\n",
        "    try:\n",
        "        prompt = f\"\"\"Generate 5 technical questions for each skill in: {tech_stack}.\n",
        "        For a candidate with {experience} years experience, create:\n",
        "        1. Conceptual question\n",
        "        2. Practical application\n",
        "        3. Coding challenge\n",
        "        4. Advanced problem\n",
        "        5. Best practices\n",
        "\n",
        "        Return ONLY the questions in this exact format:\n",
        "        [Technology]:\n",
        "        1. [Question]\n",
        "        2. [Question]\n",
        "        3. [Question]\n",
        "        4. [Question]\n",
        "        5. [Question]\"\"\"\n",
        "\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "        output = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=1200,\n",
        "            temperature=0.7,\n",
        "            do_sample=True\n",
        "        )\n",
        "        return tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "    except Exception as e:\n",
        "        return f\"⚠️ Error generating questions: {str(e)}\"\n",
        "\n",
        "def parse_questions(question_text):\n",
        "    \"\"\"Convert the generated questions into a list of (tech, question) pairs\"\"\"\n",
        "    questions = []\n",
        "    current_tech = None\n",
        "    for line in question_text.split('\\n'):\n",
        "        if line.endswith(':'):\n",
        "            current_tech = line[:-1]\n",
        "        elif line.strip() and line[0].isdigit():\n",
        "            q_num = line.split('.')[0]\n",
        "            question = line.split('. ')[1] if '. ' in line else line\n",
        "            questions.append((current_tech, question))\n",
        "    return questions\n",
        "\n",
        "def save_interview_data(user_data, qa_pairs):\n",
        "    \"\"\"Save all interview data to a JSON file\"\"\"\n",
        "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "    filename = f\"interview_{user_data['name'].replace(' ', '_')}_{timestamp}.json\"\n",
        "\n",
        "    data = {\n",
        "        \"candidate_info\": user_data,\n",
        "        \"questions_answers\": qa_pairs,\n",
        "        \"timestamp\": timestamp\n",
        "    }\n",
        "\n",
        "    with open(filename, 'w') as f:\n",
        "        json.dump(data, f, indent=2)\n",
        "\n",
        "    return filename\n",
        "\n",
        "def respond(message, chat_history):\n",
        "    global conversation_state\n",
        "\n",
        "    # Initialize conversation\n",
        "    if not chat_history:\n",
        "        conversation_state = {\n",
        "            'user_data': {},\n",
        "            'current_step': 0,\n",
        "            'generated_questions': [],\n",
        "            'question_answers': [],\n",
        "            'awaiting_answers': False\n",
        "        }\n",
        "        return information_steps[0][0], False, False\n",
        "\n",
        "    # Handle exit commands\n",
        "    if message.lower() in [\"exit\", \"quit\", \"bye\", \"goodbye\"]:\n",
        "        return \"Thank you for your time!\", False, False\n",
        "\n",
        "    # If we're collecting answers to questions\n",
        "    if conversation_state['awaiting_answers']:\n",
        "        # Store the answer with its question\n",
        "        current_q = conversation_state['generated_questions'][len(conversation_state['question_answers'])]\n",
        "        conversation_state['question_answers'].append((current_q[1], message))\n",
        "\n",
        "        # Check if all questions are answered\n",
        "        if len(conversation_state['question_answers']) == len(conversation_state['generated_questions']):\n",
        "            # Save to file\n",
        "            filename = save_interview_data(\n",
        "                conversation_state['user_data'],\n",
        "                conversation_state['question_answers']\n",
        "            )\n",
        "            return f\"✅ Thank you! All answers are been saved. Our recruitment Team will contact you shortly\", False, False\n",
        "        else:\n",
        "            # Ask next question\n",
        "            next_q = conversation_state['generated_questions'][len(conversation_state['question_answers'])]\n",
        "            return f\"{next_q[0]}:\\n{next_q[1]}\", True, True\n",
        "\n",
        "    # Normal information collection\n",
        "    current_field = information_steps[conversation_state['current_step']][1]\n",
        "    conversation_state['user_data'][current_field] = message\n",
        "\n",
        "    # Move to next question or show summary\n",
        "    if conversation_state['current_step'] < len(information_steps) - 1:\n",
        "        conversation_state['current_step'] += 1\n",
        "        return information_steps[conversation_state['current_step']][0], False, False\n",
        "    else:\n",
        "        # Generate questions\n",
        "        question_text = generate_questions(\n",
        "            conversation_state['user_data']['tech_stack'],\n",
        "            conversation_state['user_data']['experience']\n",
        "        )\n",
        "        conversation_state['generated_questions'] = parse_questions(question_text)\n",
        "\n",
        "        # Start asking questions\n",
        "        conversation_state['awaiting_answers'] = True\n",
        "        first_q = conversation_state['generated_questions'][0]\n",
        "        return f\"🔍 Technical Questions:\\n\\n{first_q[0]}:\\n1. {first_q[1]}\", True, True\n",
        "\n",
        "# Create the chat interface\n",
        "with gr.Blocks(theme=gr.themes.Soft()) as demo:\n",
        "    gr.Markdown(\"\"\"# 🧑‍💼 TalentScout Interview Assistant\"\"\")\n",
        "\n",
        "    with gr.Row():\n",
        "        with gr.Column(scale=2):\n",
        "            chatbot = gr.Chatbot(height=500)\n",
        "            msg = gr.Textbox(placeholder=\"Type your answer here...\")\n",
        "\n",
        "        with gr.Column(scale=1):\n",
        "            gr.Markdown(\"### Progress\")\n",
        "            progress_bar = gr.Slider(\n",
        "                maximum=len(information_steps) + 20,  # Extra for questions\n",
        "                interactive=False\n",
        "            )\n",
        "            with gr.Row():\n",
        "                submit_btn = gr.Button(\"📩 Submit Answer\", visible=False)\n",
        "                clear_btn = gr.Button(\"🔄 Start Over\")\n",
        "\n",
        "    # Chat functions\n",
        "    def user(user_message, history):\n",
        "        return \"\", history + [[user_message, None]]\n",
        "\n",
        "    def bot(history):\n",
        "        response, show_submit, is_question = respond(history[-1][0], history[:-1])\n",
        "\n",
        "        # Calculate progress\n",
        "        if conversation_state['awaiting_answers']:\n",
        "            progress = len(information_steps) + len(conversation_state['question_answers'])\n",
        "        else:\n",
        "            progress = conversation_state['current_step']\n",
        "\n",
        "        return history + [[None, response]], gr.update(visible=show_submit), progress\n",
        "\n",
        "    def submit_answer(history):\n",
        "        # Treat submit same as pressing enter\n",
        "        return \"\", history\n",
        "\n",
        "    # Event handlers\n",
        "    msg.submit(\n",
        "        user,\n",
        "        [msg, chatbot],\n",
        "        [msg, chatbot],\n",
        "        queue=False\n",
        "    ).then(\n",
        "        bot,\n",
        "        chatbot,\n",
        "        [chatbot, submit_btn, progress_bar]\n",
        "    )\n",
        "\n",
        "    submit_btn.click(\n",
        "        submit_answer,\n",
        "        None,\n",
        "        [msg, chatbot],\n",
        "        queue=False\n",
        "    ).then(\n",
        "        bot,\n",
        "        chatbot,\n",
        "        [chatbot, submit_btn, progress_bar]\n",
        "    )\n",
        "\n",
        "    clear_btn.click(\n",
        "        lambda: (None, False, 0),\n",
        "        None,\n",
        "        [chatbot, submit_btn, progress_bar],\n",
        "        queue=False\n",
        "    )\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 750
        },
        "id": "bT73FEbXRZsa",
        "outputId": "13f83133-e295-44bf-bc1e-14e7ae888b3d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-3-07f2117f2601>:163: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot(height=500)\n",
            "/usr/local/lib/python3.11/dist-packages/gradio/utils.py:1024: UserWarning: Expected 1 arguments for function <function submit_answer at 0x7ecb05eb6f20>, received 0.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/gradio/utils.py:1028: UserWarning: Expected at least 1 arguments for function <function submit_answer at 0x7ecb05eb6f20>, received 0.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://badcb599b4991f6ef0.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://badcb599b4991f6ef0.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rfZ0GGzITmiy"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}